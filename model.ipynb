{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import json\n",
    "import psycopg2\n",
    "from psycopg2.extras import Json\n",
    "from psycopg2 import sql\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = 'Derrick White'\n",
    "opp = 'CHA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/fw5mzqbs65d1bvrp1f8s6zc00000gn/T/ipykernel_69011/3886768634.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "/var/folders/c1/fw5mzqbs65d1bvrp1f8s6zc00000gn/T/ipykernel_69011/3886768634.py:37: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "/var/folders/c1/fw5mzqbs65d1bvrp1f8s6zc00000gn/T/ipykernel_69011/3886768634.py:55: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "# Connect to PostgreSQL and load the data\n",
    "conn = psycopg2.connect(\n",
    "    host = \"localhost\", \n",
    "    dbname = \"mnrj\", \n",
    "    user= \"postgres\", \n",
    "    password = \"gwdb\", \n",
    "    port = 5600\n",
    ")\n",
    "def load_nba(player):\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            dbname=\"mnrj\",\n",
    "            user=\"postgres\",\n",
    "            password=\"gwdb\",\n",
    "            port=5600\n",
    "        )\n",
    "        # Use the connection to execute the query\n",
    "        query = f\"SELECT * FROM nba WHERE player = '{player}';\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {e}\")\n",
    "        return None   \n",
    "def load_player_positions():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            dbname=\"mnrj\",\n",
    "            user=\"postgres\",\n",
    "            password=\"gwdb\",\n",
    "            port=5600\n",
    "        )\n",
    "        # Use the connection to execute the query\n",
    "        query = f\"SELECT * FROM latest_player_teams;\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {e}\")\n",
    "        return None\n",
    "def load_game_stats(player):\n",
    "    positions_df = load_player_positions()\n",
    "    if positions_df is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(host=\"localhost\", dbname=\"mnrj\", user=\"postgres\", password=\"gwdb\", port=5600)\n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM game_stats\n",
    "        WHERE teammates_points::jsonb ? '{player}';  -- Checking if player key exists in JSON\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "\n",
    "        # def aggregate_position_data(data, exclude_player):\n",
    "        #     position_totals = {'G': 0, 'F': 0, 'C': 0}\n",
    "        #     total_contribution = 0  # To hold the sum of all contributions for normalization\n",
    "            \n",
    "        #     # Sum values by position\n",
    "        #     for player, value in data.items():\n",
    "        #         pos = positions_df.loc[positions_df['player'] == player, 'pos'].values\n",
    "        #         if pos:\n",
    "        #             position_totals[pos[0]] += value\n",
    "        #             total_contribution += value\n",
    "            \n",
    "        #     # Subtract the contribution of the excluded player after the total is calculated\n",
    "        #     if exclude_player in data:\n",
    "        #         player_pos = positions_df.loc[positions_df['player'] == exclude_player, 'pos'].values\n",
    "        #         if player_pos:\n",
    "        #             position_totals[player_pos[0]] -= data[exclude_player]\n",
    "        #             total_contribution -= data[exclude_player]\n",
    "            \n",
    "        #     # Convert totals to percentages\n",
    "        #     if total_contribution > 0:  # Avoid division by zero\n",
    "        #         for position in position_totals:\n",
    "        #             position_totals[position] = round(position_totals[position] / total_contribution, 2)  # Round to 2 decimal places\n",
    "\n",
    "        #     return position_totals\n",
    "\n",
    "        # Aggregate data based on positions, removing the current player\n",
    "        # def aggregate_position_data(data, exclude_player):\n",
    "        #     if exclude_player in data:\n",
    "        #         del data[exclude_player]  # Remove the player from the data\n",
    "            \n",
    "        #     position_totals = {'G': 0, 'F': 0, 'C': 0}\n",
    "        #     total_contribution = 0  # To hold the sum of all contributions for normalization\n",
    "\n",
    "        #     # Sum values by position\n",
    "        #     for player, value in data.items():\n",
    "        #         pos = positions_df.loc[positions_df['player'] == player, 'pos'].values\n",
    "        #         if pos:\n",
    "        #             position_totals[pos[0]] += value\n",
    "        #             total_contribution += value\n",
    "\n",
    "        #     # Convert totals to percentages\n",
    "        #     if total_contribution > 0:  # Avoid division by zero\n",
    "        #         for position in position_totals:\n",
    "        #             position_totals[position] = round(position_totals[position] / total_contribution, 2)  # Round to 2 decimal places\n",
    "\n",
    "        #     return position_totals\n",
    "            \n",
    "        def aggregate_position_data(json_data, exclude_player):\n",
    "            if exclude_player in json_data:\n",
    "                del json_data[exclude_player]  # Remove the player from the data\n",
    "            \n",
    "            position_totals = {'G': 0, 'F': 0, 'C': 0}\n",
    "            for player, value in json_data.items():\n",
    "                pos = positions_df.loc[positions_df['player'] == player, 'pos'].values\n",
    "                if pos.size > 0:\n",
    "                    position_totals[pos[0]] += value\n",
    "\n",
    "            return position_totals\n",
    "        stats_fields = ['teammates_points', 'teammates_rebounds', 'teammates_assists', 'opponents_points', 'opponents_rebounds', 'opponents_assists',\n",
    "                        'teammates_pr','teammates_pa','teammates_ar','opponents_pr','opponents_pa','opponents_ar','teammates_pra','opponents_pra']\n",
    "        # Applying the transformation for each stats field\n",
    "        for stat_field in stats_fields:\n",
    "            df[stat_field] = df[stat_field].apply(lambda x: aggregate_position_data(x, player))\n",
    "        \n",
    "        for field in stats_fields:\n",
    "            # Expand each dictionary into separate columns\n",
    "            df_field = df[field].apply(pd.Series)\n",
    "            df_field.columns = [f\"{field}_{col}\" for col in df_field.columns]  # Rename columns to include stat field\n",
    "            df = pd.concat([df, df_field], axis=1)\n",
    "            df.drop(field, axis=1, inplace=True)  # Drop the original column\n",
    "\n",
    "\n",
    "        return df\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {e}\")\n",
    "\n",
    "nba_data = load_nba(player)\n",
    "game_stats = load_game_stats(player)\n",
    "data = nba_data.merge(\n",
    "    game_stats,\n",
    "    on= [\"team\", \"opp\",\"date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformers for both scenarios\n",
    "transformers_actual = [\n",
    "    ('actual_scaler', StandardScaler(), ['mp', 'total_score','teammates_points_F', 'teammates_points_C', 'teammates_rebounds_G',\n",
    "       'teammates_rebounds_F', 'teammates_rebounds_C', 'teammates_assists_G',\n",
    "       'teammates_assists_F', 'teammates_assists_C', 'opponents_points_G',\n",
    "       'opponents_points_F', 'opponents_points_C', 'opponents_rebounds_G',\n",
    "       'opponents_rebounds_F', 'opponents_rebounds_C', 'opponents_assists_G',\n",
    "       'opponents_assists_F', 'opponents_assists_C', 'teammates_pr_G',\n",
    "       'teammates_pr_F', 'teammates_pr_C', 'teammates_pa_G', 'teammates_pa_F',\n",
    "       'teammates_pa_C', 'teammates_ar_G', 'teammates_ar_F', 'teammates_ar_C',\n",
    "       'opponents_pr_G', 'opponents_pr_F', 'opponents_pr_C', 'opponents_pa_G',\n",
    "       'opponents_pa_F', 'opponents_pa_C', 'opponents_ar_G', 'opponents_ar_F',\n",
    "       'opponents_ar_C', 'teammates_pra_G', 'teammates_pra_F',\n",
    "       'teammates_pra_C', 'opponents_pra_G', 'opponents_pra_F',\n",
    "       'opponents_pra_C']),  # Example features\n",
    "    ('categorical', OneHotEncoder(handle_unknown='ignore'), ['opp'])\n",
    "]\n",
    "\n",
    "transformers_percentage = [\n",
    "    ('percentage_scaler', MinMaxScaler(), ['teammates_points_F', 'teammates_points_C', 'teammates_rebounds_G',\n",
    "       'teammates_rebounds_F', 'teammates_rebounds_C', 'teammates_assists_G',\n",
    "       'teammates_assists_F', 'teammates_assists_C', 'opponents_points_G',\n",
    "       'opponents_points_F', 'opponents_points_C', 'opponents_rebounds_G',\n",
    "       'opponents_rebounds_F', 'opponents_rebounds_C', 'opponents_assists_G',\n",
    "       'opponents_assists_F', 'opponents_assists_C', 'teammates_pr_G',\n",
    "       'teammates_pr_F', 'teammates_pr_C', 'teammates_pa_G', 'teammates_pa_F',\n",
    "       'teammates_pa_C', 'teammates_ar_G', 'teammates_ar_F', 'teammates_ar_C',\n",
    "       'opponents_pr_G', 'opponents_pr_F', 'opponents_pr_C', 'opponents_pa_G',\n",
    "       'opponents_pa_F', 'opponents_pa_C', 'opponents_ar_G', 'opponents_ar_F',\n",
    "       'opponents_ar_C', 'teammates_pra_G', 'teammates_pra_F',\n",
    "       'teammates_pra_C', 'opponents_pra_G', 'opponents_pra_F',\n",
    "       'opponents_pra_C']),  # Example features\n",
    "    ('actual_scalar', StandardScaler(), ['mp','total_score']),\n",
    "    ('categorical', OneHotEncoder(handle_unknown='ignore'), ['opp'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 6.417271616504946\n",
      "Root Mean Squared Error: 6.665055013526395\n",
      "[16.81 17.02 19.19 15.95 14.22 12.04 14.69 17.52 16.38 13.87 16.83 15.68\n",
      " 14.45 14.07 14.88 15.58 17.43 16.84 18.5  13.12 21.36 19.22 15.91 17.36]\n",
      "16.301666666666666\n"
     ]
    }
   ],
   "source": [
    "df = data\n",
    "\n",
    "# Specify features and target\n",
    "features = ['opp', 'mp', 'total_score','teammates_points_F', 'teammates_points_C', 'teammates_rebounds_G',\n",
    "       'teammates_rebounds_F', 'teammates_rebounds_C', 'teammates_assists_G',\n",
    "       'teammates_assists_F', 'teammates_assists_C', 'opponents_points_G',\n",
    "       'opponents_points_F', 'opponents_points_C', 'opponents_rebounds_G',\n",
    "       'opponents_rebounds_F', 'opponents_rebounds_C', 'opponents_assists_G',\n",
    "       'opponents_assists_F', 'opponents_assists_C', 'teammates_pr_G',\n",
    "       'teammates_pr_F', 'teammates_pr_C', 'teammates_pa_G', 'teammates_pa_F',\n",
    "       'teammates_pa_C', 'teammates_ar_G', 'teammates_ar_F', 'teammates_ar_C',\n",
    "       'opponents_pr_G', 'opponents_pr_F', 'opponents_pr_C', 'opponents_pa_G',\n",
    "       'opponents_pa_F', 'opponents_pa_C', 'opponents_ar_G', 'opponents_ar_F',\n",
    "       'opponents_ar_C', 'teammates_pra_G', 'teammates_pra_F',\n",
    "       'teammates_pra_C', 'opponents_pra_G', 'opponents_pra_F',\n",
    "       'opponents_pra_C']\n",
    "target = 'pts'\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=20)\n",
    "\n",
    "preprocessor_actual = ColumnTransformer(transformers=transformers_actual)\n",
    "preprocessor_percentage = ColumnTransformer(transformers=transformers_percentage)\n",
    "\n",
    "# Create Pipelines\n",
    "pipeline_actual = Pipeline([\n",
    "    ('preprocessor', preprocessor_actual),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "pipeline_percentage = Pipeline([\n",
    "    ('preprocessor', preprocessor_percentage),\n",
    "    ('regressor', RandomForestRegressor())])\n",
    "\n",
    "# Fit the model\n",
    "pipeline_actual.fit(X_train, y_train)\n",
    "pipeline_percentage.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred_a = pipeline_actual.predict(X_test)\n",
    "y_pred_p = pipeline_percentage.predict(X_test)\n",
    "mse_a = mean_squared_error(y_test, y_pred_a)\n",
    "mse_p = mean_squared_error(y_test, y_pred_p)\n",
    "print(f'Root Mean Squared Error: {math.sqrt(mse_a)}')\n",
    "print(f'Root Mean Squared Error: {math.sqrt(mse_p)}')\n",
    "print(y_pred_a)\n",
    "print(statistics.mean(y_pred_p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
